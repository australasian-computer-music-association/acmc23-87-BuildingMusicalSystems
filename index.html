<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2023-07-13" />
  <title>Building Musical Systems: An Approach Using Real-Time Music Information Retrieval Tools</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="templates/pandoc.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header id="title-block-header">
<h1 class="title">Building Musical Systems: An Approach Using Real-Time
Music Information Retrieval Tools</h1>
<p class="author">Austin Franklin
 (Louisiana State University)
 (<a href="https://orcid.org/https://orcid.org/0000-0001-7194-0807">ORCID</a>)
</p>
<p class="date">Submitted: 2023-07-13</p>
<p class="date">Published: 2023-10-09</p>
</header>
<p class="abstract"><em>Abstract:</em> <p>This research presents a new
approach to computer automation, termed Reflexive Automation, through
the implementation of novel real-time music information retrieval
algorithms developed for this project. It introduces the development of
the PnP.Maxtools library, a `plug and play’ set of filters, subjective
timbral descriptors, audio effects, and other objects developed in
Cycling ’74 Max that is designed as a framework for building musical
systems for music using live electronics without the use of external
controllers or hardware. The library is designed to take incoming audio
from a microphone, analyze it, and use the analysis to control an audio
effect on the incoming signal in real-time. This approach to automation
and library together represents the first defined method for performance
and composition practice using music information retrieval
algorithms.</p></p>
<h1 data-number="1" id="introduction"><span
class="header-section-number">1</span> Introduction</h1>
<p>Automation in computer music is traditionally understood as an
offline approach to controlling musical parameters over time, while
Reflexive Automation views computer automation in a manner like
classification and regression algorithms found in machine learning
systems, which automate outputs based on training data and eventually
inputs. However, reflexive systems do not require training. Instead,
they respond to specific timbral features of live inputs through
real-time analysis determined by the music information retrieval
algorithm(s) implemented in the system. MIR algorithms use indirect
acquisition of performance parameters from the incoming audio signal
with a microphone. This is a non-invasive method (as opposed to direct
signal acquisition which involves physical augmentation using sensors)
for extracting parameters <span class="citation"
data-cites="indirect">(Traube et al., 2003)</span>. By explicitly
mapping the output of a real-time MIR algorithm to control an audio
effect that is processing the same signal used for analysis, Reflexive
Automation provides a blueprint for creating meaningful interactive
musical systems between performers and computers.</p>
<p>Automation in music can refer to many things, but generally refers to
any system that moves or acts of itself. Automated composition refers to
a “formal process to make music with minimal human intervention” <span
class="citation" data-cites="alpern">(Alpern, 1995)</span>. This type of
composition removes the composer from a large portion of compositional
process; the composer only needs to invent the musical kernel for the
composition. Many of the automated composition techniques utilize
computers to generate choices based on predetermined rules or
algorithms, such as the Illiac Suite by Lejaren Hiller and Leanoard
Isaacson at the University of Illinois in 1956. Others, such as the
music of Iannis Xenakis employ stochastic processes for generating
various musical parameters. The difference between these two
methodologies, rule-based and stochastic, is that in rule-based
composition the computer makes decisions on behalf of the composer. On
the other hand, stochastic processes only “aid the composer by virtue of
its high-speed computations” <span class="citation"
data-cites="cope">(Cope, 1976)</span>. A third category are systems that
use artificial intelligence. These systems are like rule-based systems
in that they are programs based on pre-defined datasets used for
training. However, they have the added capacity to define their own
rules while simultaneously providing high-speed computational aid.</p>
<p>In live electronic music today, automatic listening programs are
popular for changing musical parameters. However, very few resources
exist for aiding composers attempting to create with these tools. Many
composers have developed their own complex approaches, such as Voyager
by George Lewis <span class="citation" data-cites="voyager">(Lewis,
2000)</span>, but still there seems to be very little or no taxonomy of
automatic listening techniques. There are several real-time MIR tools
such as FluCoMa <span class="citation" data-cites="flucoma">(Zbyszynski,
2019)</span>, FTM/Gabor Object Library <span class="citation"
data-cites="gabor">(Schnell &amp; Schwarz, 2005)</span>, MuBu <span
class="citation" data-cites="mubu">(Schnell et al., 2009)</span>, and
fiddle~ and bonk~ by Miller Puckette <span class="citation"
data-cites="fiddle">(Puckette et al., 1998)</span> in Max. Malt et
al. developed Zsa.Descriptors, a collection of real-time sound
descriptors for analysis in Max that includes models that can detect
objective features such as the spectral centroid, spectral spread, and
spectral roll-off using an FFT. These libraries do not provide
additional filters, controls, or effects that can be used in conjunction
with MIR tools. They require fluency in Max and rely heavily on built in
objects. Additionally, Malt et al. describes the ``lack of knowledge of
the relationships between descriptors and the pertinent perceptual
characteristics of the sound for use in musical composition” as a factor
in why MIR algorithms are rarely used in music composition <span
class="citation" data-cites="zsa">(Malt &amp; Jourdan, 2008)</span>.
Musicians can sometimes further complicate this relationship by
employing broad language. Another factor is that one descriptor is
generally not useful for characterizing changes to complex spectra. The
question of how to meaningfully and accurately describe a sound,
purposefully retrieve data from that sound that relates to that
description, and then use that data for a musical goal quickly becomes
very challenging.</p>
<p>This research presents an approach and library for understanding and
implementing real-time automatic systems, termed Reflexive systems, by
framing automation as an automatic response to external stimuli, or a
``reflex,” rather than a user-supervised approach for producing change
over time. Reflexive Automation, then, can be defined as any system that
moves or acts of itself in relation to an external force. As opposed to
automatic listening programs, reflexive systems are more concerned with
the response from the system: the act of doing versus the act of
listening. The degree of automatism or agency that exists within these
systems is due to explicit mapping choices made by the composer.
However, from the perspective of the performer it appears that the live
electronics are fully autonomous. Reflexive Automation is intended to be
used for creative purposes and may include for many alterations; it is a
philosophy more than a recipe. It addresses the challenges stated above
not only with new MIR tools but also with a conceptual framework for
using these tools within a variety of musical contexts.</p>
<h1 data-number="2" id="reflexive-automation"><span
class="header-section-number">2</span> Reflexive Automation</h1>
<p>It is perhaps easiest to understand Reflexive Automation as the
simultaneous use of automation, mapping, and music information retrieval
in a system, the output of which results in a reflex that is the direct
result of the input. The term reflex is used to communicate to
performers that gesture, motor control, and physicality typically
associated with many years of practice on an instrument are valuable and
have expressive potential within these systems. Reflexive systems use
information derived using MIR techniques from an audio signal and map it
to audio effects parameters which are processing the same signal used
for analysis. The way the effects or parameters change as a result of
the input is determined by the mapping strategies utilized in the
system. However, they typically fall into one of the following
categories: one-to-one, one-to-many, many-to-one <span class="citation"
data-cites="hunt">(Hunt &amp; Wanderley, 2002)</span>.</p>
<figure>
<img src="media/patellarsignal.png"
alt="A simple biological reflexive system (left) and a digital reflexive system (right)" />
<figcaption aria-hidden="true">A simple biological reflexive system
(left) and a digital reflexive system (right)</figcaption>
</figure>
<p>The term Reflexive Automation is, in part, inspired by a biological
reflexive system known as the patellar reflex test (Figure 1). In this
analogy, the plexor is the incoming signal, or the action from which
information is derived. Once the information is retrieved through
contact with sensory neurons in the patellar, it is transmitted through
the neuromuscular system and triggers an impulse in the motor neurons in
the quadriceps muscles. The reflex arc occurs at the level of the spinal
cord, meaning that the associated movement occurs without involvement
from the brain. The result is a short jerk in the knee of the patient
that happens automatically and involuntarily. It is an autonomous
reaction that occurs as the direct result of a system of mapping of
neurons in the body for the purpose of responding to external stimuli.
In the context of computer music with live instruments, this conceives
of audio processing as a gestural analog to the gestures of performers,
as extensions of the sounds of their acoustic instruments. Like the
rapid movement of the knee, the effect on the resulting sound changes as
a response to the input, to the specific and unique sounds of the
performance. The system must be designed to require no additional
involvement or supervision from a controlling force to operate
internally on the system, such as the composer or additional performers.
In other words, it requires the absence of the involvement of the
`brain.’</p>
<p>Reflexive systems can already be found in electronics and audio
systems. For example, a microphone that clips does so in response to an
input amplitude level that is too high to properly record. An
over-driven amplifier results in distortion due to high gain levels that
exceed voltage capacity. Systems such as these respond to changes to the
input, regardless of whether the transfer functions of these systems are
linear or nonlinear. All these systems utilize music information
retrieval, albeit in a very non-purposeful manner, and a system of
mapping that determines how the system automates or responds to the
input.</p>
<h2 data-number="2.1" id="affordances"><span
class="header-section-number">2.1</span> Affordances</h2>
<p>Much live electronic music today requires performers to utilize
several controllers, such as foot pedals, while playing their
instruments. The often overly elaborate technical requirements and
physical demands of live electronic music can create barriers for
musicians. By relying on specific qualities of the sound of an
instrument as the only external controller, the composer is inclined to
become more sensitive to the technical demands of the performance and
the performer will have a better working knowledge, through their
instrument, of the system for which they are expected to interact. The
only necessary hardware requirements currently are a laptop, audio
interface, microphone, and speakers.</p>
<p>Computer programs used for live performance often require the
performer to synchronize with a variety of cues, perform with a
stopwatch and adjust the speed of their performance to arrive at a
certain place in the score at a specific time, or perform alongside a
track for which sometimes vague or unspecific graphic notation exists.
These aspects of live electronic music can place the role of the
performer as secondary, or as accompaniment, to the electronics. Failure
with regards to live electronic performance in this context often means
not performing the part “correctly” with regards to the score — meeting
all of the synchronization points, alignment, etc. — and does not often
refer to the subtleties of performance interpretation. Failure with
regards to performing Chopin, for example, is related to the specifics
of the interpretation, whether by tempo fluctuations (rubato), dynamic
fluctuations, or other subtle distinctions not expressly notated in the
score.</p>
<p>With regards to timbre, some notation exists such as sul ponticello
and sul tasto on string instruments. Wind instruments can utilize a
range of effects such as harmonics, multiphonics, noisier sounds such as
flutter tonguing, and singing and playing simultaneously. Each of these
has their own unique spectra and notation. Outside of this, performers
have a wide range of terms that they use to classify the types of timbre
that can be produced on their instruments, such as brightness and
warmth. While these are perceived characteristics of timbre as opposed
to a purely objective timbral analysis, they provide a useful common
language for notation. When building reflexive systems, it is not
necessary to include a separate notation for electronic elements in the
score alongside the acoustic instruments, because the notation for the
performer and the resulting electronic manipulation can be written using
the same symbols or text. Text in the front matter of the score will
suffice, in most cases, for conveying the behavior of the electronics to
the performer.</p>
<h1 data-number="3" id="related-concepts"><span
class="header-section-number">3</span> Related Concepts</h1>
<p>Reflexive systems can be further understood in terms of cybernetics,
a field concerned with causal circular feedback systems <span
class="citation" data-cites="cybernetics">(Steer, 1952)</span>. Robert
Wiener cites the steering of a ship as ``one of the earliest and
best-developed forms of feedback mechanisms,” and the broad context for
understanding these systems has made them useful in many disciplines
<span class="citation" data-cites="wiener">(Wiener, 2019)</span>. The
notion of the sensor and controller is important to cybernetic systems.
The sensor compares what is happening to a system with a standard of
what should be happening, while the controller adjusts the behavior of
the system. In the ship steering example, the helmsman adjusting the
helm is the controller and the direction the ship takes based on that
control input, water disturbance, crosswinds, and tide is the sensor.
The helmsman is not in direct control of the ship, but rather in control
of a complex system through a narrow set of inputs. The
sensor-controller paradigm within cybernetics is what creates feedback
in a variety of contexts. A simple cybernetic system is shown in Figure
2.</p>
<figure>
<img src="media/cybernetic.png" alt="A simple cybernetic system" />
<figcaption aria-hidden="true">A simple cybernetic system</figcaption>
</figure>
<p>Reflexive systems create feedback loops through auditory responses by
the performer based on outputs which are mediated by the specifics of
the system, i.e., what descriptors, controls, and effects are
implemented. The notion of listening and responding in real-time is an
inherent aspect of music, particularly in improvisation. Like the
steering of a ship, the choice of input must be made based on the
current output from the system and how the sound is transformed by
internal processes and subsystems.</p>
<p>Building reflexive systems relates to paradigms found in software
design for building applications. One example in web development is the
Model-View-Controller Design Pattern <span class="citation"
data-cites="mvc">(Bucanek, 2009)</span>. Software design patterns are
templated, reusable solutions to commonly occurring problems within a
given context in software design. On their own they cannot be
transformed directly into source or machine code, but instead are
blueprints for solving problems in many different applications and
systems. Whereas most patterns address specific problems, the
Model-View-Controller design pattern describes the architecture of a
system of objects. The less clearly defined nature of the
Model-View-Controller design pattern allows for broad applications in
computer systems. It also plays an important role within information
visualization <span class="citation" data-cites="sdp">(Heer &amp;
Agrawala, 2006)</span>. The Model-View-Controller design pattern is
shown in Figure 3.</p>
<figure>
<img src="media/mvc-process.png"
alt="The Model-View-Controller software design pattern" />
<figcaption aria-hidden="true">The Model-View-Controller software design
pattern</figcaption>
</figure>
<p>The user is crucial in both Reflexive Automation and the
Model-View-Controller design pattern to interact with the system. In
reflexive systems, the user controls the model with their instrument
while the descriptors and other specific aspects of the system are
hidden from the performers’ view. The user in the Model-View-Controller
design pattern can likewise input control data in various formats. The
benefit of this pattern is that the user is presented with abstracted
controls and is not required to interact with the full complexity of the
system. Furthermore, the PnP.Maxtools library contains categories of
objects that function together as a framework like design patterns. It
is intended to provide a template for building reflexive systems, and
the interchangeable nature of objects within each category facilitates
experimentation and prototyping.</p>
<h1 data-number="4" id="pnp.maxtools-overview"><span
class="header-section-number">4</span> PnP.Maxtools Overview</h1>
<p>The PnP.Maxtools library includes filters, timbral descriptors,
controls, and effects that are designed for the real-time implementation
of reflexive systems for music composition and improvisation. While most
libraries in Max contain a set of similar objects based around a single
tool or technique and provide many functions with that tool, the
PnP.Maxtools package is designed as a framework using modular
categories. While most objects in the package are novel, the effects are
commonplace within the computer music paradigm. These include plate
reverb, distortion, variable delay, and a real-time implementation of
the famous Karplus-Strong plucked string algorithm <span
class="citation" data-cites="pluck">(Karjalainen et al., 1998)</span>.
Figure 4 shows one possible configuration for the package categories.
Each object from a category can be used interchangeably with others from
the same category to create numerous interactions and resulting sounds.
Additionally, more configurations of these categories are possible
depending upon the mapping strategies, effects, and other objects
used.</p>
<figure>
<img src="media/signal-framework.png"
alt="The PnP.Maxtools category framework" />
<figcaption aria-hidden="true">The PnP.Maxtools category
framework</figcaption>
</figure>
<p>The goal behind creating categories of objects is to provide a
strategy for building musical systems for music with live electronics.
Many packages in Max provide a set of limited tools, but they often
heavily rely upon the vast selection on built-in objects in Max. While
this is generally considered a feature of third-party libraries rather
than a bug, it can often be overwhelming for beginners and students. The
categorical framework and `plug and play’ style of the package is
designed to be as user-friendly for Max users as possible. It is
particularly geared towards beginning users and students, requiring no
additional objects outside of the package for building reflexive
systems. This is aided by additional package features, such as default
arguments for all objects for general purposes and a preset frame size
of 2048 samples for all descriptors which utilize an FFT. Additionally,
the package launcher contains a demo where package objects can be
randomly configured and techniques such as creating cooperative
descriptors, effects chaining, and event detection functions are
described in detail. Examples can be tested from within the demo then
copied and pasted into another patcher and modified further. There are
43 objects in total.</p>
<h2 data-number="4.1" id="filters"><span
class="header-section-number">4.1</span> Filters</h2>
<p>The filters provide pre-processing functionality and are intended to
be used to restrict the range of frequencies or remove unwanted sounds
from an audio signal before analysis. All of the filters are implemented
using the pfft~ object in Max using a frame size of 2048 samples. These
objects function by gating frequency bins, allowing certain ones to pass
through the pfft~ object unaffected while multiplying the real and
imaginary values of other bins by zero. In the pfft~ object subpatcher,
the furthest right outlet of the fftin~ object indexes the signal bin
number and corresponds to the real and imaginary signals for that bin.
Using a few boolean operators such as &lt;~, &lt;~, ==~, etc. (“~”
denotes signal operators), it is possible to isolate any number of bins
or bin regions for calculation.</p>
<h2 data-number="4.2" id="descriptors"><span
class="header-section-number">4.2</span> Descriptors</h2>
<p>The timbral descriptors in the PnP.Maxtools package are a set of
high- and low-level objective and subjective descriptors, the efficacy
of which was evaluated through a study to determine the degree to which
the output from these models align with perceived characteristics of
that sound <span class="citation" data-cites="austin">(Franklin,
2022)</span>. Additionally, evaluation of the PnP.Maxtools package was
done using the Cranfield model for evaluating information retrieval
systems <span class="citation" data-cites="cranfield">(Moffat et al.,
2015)</span>. One of the benefits of the PnP.Maxtools descriptors is
that they output a normalized floating-point value between 0-1, where 0
corresponds to low perceived characteristics and 1 high perceived
characteristics. With this standardization, it is possible to create
cooperative descriptors, or descriptors created through the
implementation of more than one descriptor simultaneously. For example,
a particular sound may be best described as having both significant
spectral roughness and depth. This will distinguish the sound from
others which may correlate with significant spectral roughness or depth,
but not both. This may be calculated using: <em><span
class="math inline"><em>D</em>(<em>n</em>) = (<em>R</em><em>o</em><em>u</em><em>g</em><em>h</em><em>n</em><em>e</em><em>s</em><em>s</em>*0.6) + (<em>D</em><em>e</em><em>p</em><em>t</em><em>h</em>*0.4)</span></em>
where <em><span class="math inline">(<em>n</em>)</span></em> is the
value at the <em><span
class="math inline"><em>n</em><em>t</em><em>h</em></span></em> index or
frame. This section presents the newest additions to Max
environment.</p>
<h3 data-number="4.2.1" id="pnp.boominess"><span
class="header-section-number">4.2.1</span> pnp.boominess~</h3>
<p>A boomy sound is one that conveys a sense of loudness, depth, and
resonance. Several boominess calculations have been proposed, such as
the Booming Index as described by Shigeko Hatano and Takeo Hashimoto in
“Booming Index as a Measure for Evaluating Booming Sensation” <span
class="citation" data-cites="booming-index">(Hatano &amp; Hashimoto,
2000)</span>. The method of calculation Hatano et al. propose makes use
of the order analysis of a sound. From this the fundamental frequency
and harmonics can be determined and the loudness of these calculated. In
Max, pnp.boominess~ calculates the apparent boominess of an incoming
signal based on the sharpness model described by Fastl and Zwicker in
“Psychoacoustics: Facts and Models” <span class="citation"
data-cites="boomy-sharpy">(Zwicker &amp; Fastl, 2013)</span>. However,
Fastl et al. proposes that boominess is a measure of the low frequency
content of a sound rather than high frequencies; the greater the
proportion of low frequencies the greater the ‘booming’ sound. So
boominess can be considered the opposite of the sensation of sharpness.
Using Fastl and Zwicker’s approach boominess can be calculated as:</p>
<figure>
<img src="media/boomy.png"
alt="Weighting, g(z), as a function of critical band rate for boominess" />
<figcaption aria-hidden="true">Weighting, g(z), as a function of
critical band rate for boominess</figcaption>
</figure>
<p>where <em>N</em> is the total spectral loudness, <em>g(z)</em> is the
weighting factor for boominess as a function of the critical-band rate,
and <em>dz</em> is a scaling factor. Only for critical-band rates less
than 22 bark does the weighting factor increase from unity to a value of
4.5 at the end of the critical-band rate near 0 bark (Figure 5).</p>
<h3 data-number="4.2.2" id="pnp.depth"><span
class="header-section-number">4.2.2</span> pnp.depth~</h3>
<p>A deep sound is one that conveys the sense of having been made far
down below the surface of its source. Whilst the attribute of depth is
mentioned in several academic papers, only AudioCommons has proposed a
model and suggested acoustic correlates <span class="citation"
data-cites="acrelease">(Pearce &amp; Brookes, 2019)</span>. However, an
online experiment by Cartwright et al. called Social-EQ asked subjects
to submit a timbral descriptor together with an appropriate setting on a
40-band graphic equalizer that demonstrates that descriptor <span
class="citation" data-cites="cartwright">(Cartwright &amp; Pardo,
2013)</span>. Six subjects chose to submit the term deep. The 40-band
equalization treatment submitted by each subject is shown in the Figure
6. The mean equalization of all subjects, and 95% confidence intervals,
are shown in the thicker black line.</p>
<figure>
<img src="media/depth.png"
alt="Social-EQ graphic equalizer representing the timbral descriptor deep" />
<figcaption aria-hidden="true">Social-EQ graphic equalizer representing
the timbral descriptor deep</figcaption>
</figure>
<p>There is a clear trend in Figure 8 that shows that all subjects’ EQ
treatments emphasized the low frequency content of the signal. Since
there is a large degree of commonality in these EQ treatments, it is
likely that timbral depth is related to having emphasized low frequency
content. Pearce suggests that a suitable model for depth would be to
analyze: 1) the spectral centroid of the lower frequencies (energy
pulling towards the low-end); 2) the proportion of low frequency energy;
and/or 3) the low-frequency limit of the audio extract (the low
frequency roll-on). The pnp.depth~ implementation is a direct
implementation of the model described by Pearce. It includes calculation
of the lower spectral centroid and the ratio of energy between 30Hz and
200Hz compared to all energy up to the Nyquist frequency. The lower
spectral centroid is calculated using:</p>
<p>where <em><span
class="math inline"><em>n</em>(<em>ω</em>)</span></em> is the bin number
relating to frequency <em><span
class="math inline"><em>ω</em></span></em>, <em><span
class="math inline"><em>f</em>(<em>n</em>)</span></em> is the frequency
of the <em><span class="math inline"><em>n</em></span>th</em> bin, and
<em><span class="math inline"><em>x</em>(<em>n</em>)</span></em> is the
magnitude of the <em><span class="math inline"><em>n</em></span>th</em>
bin. The model also calculates the ratio of energy between 30Hz and
200Hz compared to all energy up to the Nyquist frequency:</p>
<p>where <em>n(Nyquist)</em> is the frequency relating to the Nyquist
frequency.</p>
<h3 data-number="4.2.3" id="pnp.flatness"><span
class="header-section-number">4.2.3</span> pnp.flatness~</h3>
<p>The pnp.flatness~ object calculates the spectral flatness of each FFT
frame. The spectral flatness is used to quantify the tonal quality,
i.e., how tone-like the sound is as opposed to being noise-like <span
class="citation" data-cites="flatness">(Izmirli, 2000)</span>. Spectral
flatness is defined by the ratio of the geometric mean to the arithmetic
mean of the power spectral density components in each critical band. It
is calculated as:</p>
<p>where <em><span
class="math inline"><em>n</em>(<em>ω</em>)</span></em> is the bin number
relating to frequency <em><span
class="math inline"><em>ω</em></span></em>, <em><span
class="math inline"><em>n</em>(<em>N</em><em>y</em><em>q</em><em>u</em><em>i</em><em>s</em><em>t</em>)</span></em>
is the frequency relating to the Nyquist frequency, and <em><span
class="math inline"><em>x</em>(<em>n</em>)</span></em> is the magnitude
of the <em>n</em> bin. The Max implementation is a modified
implementation of the model by Izmirli that performs calculations on
each bin within a frame rather than on critical bands.</p>
<h3 data-number="4.2.4" id="pnp.hardness"><span
class="header-section-number">4.2.4</span> pnp.hardness~</h3>
<p>A hard sound is one that conveys the sense of having been made (i) by
something solid, firm, or rigid; or (ii) with a great deal of force.
Although no explicit model of hardness exists in the literature, there
is an indication that the attack and the spectral content of the attack
determine the apparent hardness <span class="citation"
data-cites="acfirst">(Andy Pearce, 2017)</span>. Research by Williams
suggests that the onset portion of a sound determines the perception of
hardness <span class="citation" data-cites="williams">(Williams,
2010)</span>. Additionally, Freed presents a model of mallet hardness
perception for single percussive sounds with respect to four acoustic
correlates: 1) spectral mean level (a form of long-term average
spectrum, LTAS); 2) spectral level slope; 3) spectral centroid mean
(mean spectral centroid over time, measured on the bark scale); and 4)
spectral centroid TWA (time weighted mean of the spectral centroid)
<span class="citation" data-cites="freed">(Freed, 1990)</span>. A model
of hardness was developed by Pearce et al. which employs three metrics:
(i) attack time; (ii) attack gradient; and (iii) spectral centroid of
attack. The model calculates the attack gradient (difference in
amplitudes of the attack start and end levels divided by the linear
attack time) of the sound using a fixed attack time of 125ms:</p>
<p>where is the amplitude relating to the attack of the signal. The
attack spectral centroid is then calculated over the first 200ms before
the attack and 125ms after the attack start, or until the next onset
time if it happens before 125ms:</p>
<p>where <em><span
class="math inline"><em>n</em>(<em>ω</em>)</span></em> is the bin number
relating to frequency <em><span
class="math inline"><em>ω</em></span></em>, <em><span
class="math inline"><em>f</em>(<em>n</em>)</span></em> is the frequency
of the <em><span class="math inline"><em>n</em></span>th</em> bin, and
<em><span class="math inline"><em>x</em>(<em>n</em>)</span></em> is the
magnitude of the <em><span class="math inline"><em>n</em></span>th</em>
bin.</p>
<p>The implementation of hardness in Max is a modified version of the
model proposed by Pearce et al. that calculates the attack gradient
using a fixed attack time of 125 and the brightness and depth of the
attack using the PnP.Maxtools descriptors previously described over the
first 10ms before the attack and 125ms after the attack start <span
class="citation" data-cites="acfirst">(Andy Pearce, 2017)</span>. This
is done to capture as much of the period before the onset as possible
without adding noticeable latency. The attack gradient, depth, and
brightness are then scaled so the maximum value that can be returned
from the model is 1.</p>
<h3 data-number="4.2.5" id="pnp.metallic"><span
class="header-section-number">4.2.5</span> pnp.metallic~</h3>
<p>The pnp.metallic~ object calculates the probability that an incoming
sound is produced by a metallic source. Aramaki et al. identifies four
timbre descriptors that are relevant signal features for the
discrimination between sound categories: attack time, spectral
bandwidth, roughness, and normalized sound decay. These are used to
determine whether characteristics of a sound resemble that of sounds
made by metallic objects. In general, metallic sounds contain rich and
complex spectra relative to other sounds, such as those made by wooden
or glass object <span class="citation" data-cites="controlling">(Aramaki
et al., 2010)</span>. First, the spectral standard deviation is
calculated with the equation:</p>
<p>where <em><span class="math inline"><em>μ</em></span></em> is the
spectral centroid in hertz, <em><span
class="math inline"><em>n</em>(<em>ω</em>)</span></em> is the bin number
relating to frequency <em><span
class="math inline"><em>ω</em></span></em>, <em><span
class="math inline"><em>f</em>(<em>n</em>)</span></em> is the frequency
of the <em><span class="math inline"><em>n</em></span>th</em> bin, and
<em><span class="math inline"><em>x</em>(<em>n</em>)</span></em> is the
magnitude of the <em><span class="math inline"><em>n</em></span>th</em>
bin. The normalized decay time is calculated by taking the absolute of
the Hilbert Transform of the signal, followed by a low pass second-order
Butterworth filter with a cut-off frequency of 50Hz <span
class="citation" data-cites="hilbert">(Johansson, 1999)</span>. The
logarithm of this is taken after adding 1 to the result, which ensures
that the logarithm of 0 is never calculated. This is expressed with the
equation:</p>
<p>where <em><span class="math inline"><em>x</em></span></em> is the
audio signal, <em><span
class="math inline"><em>H</em>(<em>x</em>)</span></em> is the Hilbert
Transform of <em>x</em>, and <em><span
class="math inline"><em>F</em>(<em>x</em>)</span></em> represents
filtering of the signal. The roughness is then calculated using the
method proposed by Vassilakis described below <span class="citation"
data-cites="roughness">(Vassilakis &amp; Fitz, 2007)</span>. The
implementation in Max is a direct implementation of the model proposed
by Aramaki et al., where the metallic probability of each FFT frame is
calculated. Only the attack time was omitted from the calculation
because the attack time is dependent upon the detection and analysis of
onsets in the signal, making resonance and the gradual decay metallic
sound more difficult to detect.</p>
<h3 data-number="4.2.6" id="pnp.roughness"><span
class="header-section-number">4.2.6</span> pnp.roughness~</h3>
<p>A rough sound is one that has an uneven or irregular sonic texture.
The pnp.roughness~ object calculates the apparent roughness of an
incoming audio signal using an FFT and Gen~ using the method proposed by
Vassilakis <span class="citation" data-cites="roughness">(Vassilakis
&amp; Fitz, 2007)</span>. The term auditory roughness was first
introduced in the literature by Helmholtz to describe the buzzing,
harsh, raspy sound quality of narrow harmonic intervals <span
class="citation" data-cites="helmholtz">(Helmholtz, 2009)</span>. The
dimension of dissonance correlating best with auditory roughness has
been termed sensory or tonal dissonance <span class="citation"
data-cites="tonal">(Plomp &amp; Levelt, 1965)</span> or auditory
dissonance <span class="citation" data-cites="consonance">(Hutchinson
&amp; Knopoff, 1978)</span>. The Vassilakis Roughness model detects all
peaks in the frequency spectrum for each frame where: (i) the magnitude
of the frequency bin is greater than 0.01; (ii) the magnitude of the
previous and next bins are less than the current bin; and (iii) in the
frequency range between successive peaks the magnitude drops at least
0.01 below the magnitude of the lower peak. For each pair of peaks
within a frame, the roughness is calculated with the equation:</p>
<p>with:</p>
<p>where <em><span class="math inline"><em>r</em></span></em> is the
roughness, <em><span
class="math inline"><em>A</em><sub><em>m</em><em>a</em><em>x</em></sub></span></em>
and <em><span
class="math inline"><em>A</em><sub><em>m</em><em>i</em><em>n</em></sub></span></em>
are the maximum and minimum magnitudes of the pair of peaks, and
<em><span
class="math inline"><em>f</em><sub><em>m</em><em>a</em><em>x</em></sub></span></em>
and <em><span
class="math inline"><em>f</em><sub><em>m</em><em>i</em><em>n</em></sub></span></em>
are the maximum and minimum frequencies of the two peaks
respectively.</p>
<h3 data-number="4.2.7" id="pnp.sharpness"><span
class="header-section-number">4.2.7</span> pnp.sharpness~</h3>
<p>A sharp sound is one that suggests it might cut if it were to take on
physical form. The pnp.sharpness~ object calculates the apparent
sharpness of an incoming signal based on the model described by Fastl
and Zwicker <span class="citation" data-cites="boomy-sharpy">(Zwicker
&amp; Fastl, 2013)</span>. Closely related to sharpness, however
inversely, is a sensation called sensory pleasantness. Fastl et
al. defines a sound of sharpness 1 acum as “a narrow band noise one
critical band wide at a centre frequency of 1kHz having a level of
60dB.” However, sharpness is a metric which has not yet been
standardized. Consequently, there are several methods to calculate the
metric including: Von Bismarck’s method <span class="citation"
data-cites="von">(Bismarck, 1974)</span> introduces the idea of a
weighted first moment calculation, Aures’s method <span class="citation"
data-cites="aures">(Aures, 1985)</span> is a modified version of Von
Bismarck’s equation, and Fastl and Zwicker’s method which is a version
of Von Bismarck’s equation with a modified weighting curve. Like
boominess, sharpness has been used to partially quantify sound quality
in examples such as measuring engine noise, and some domestic appliances
such as vacuum cleaners and hair dryers. It has also been used in the
calculation of a sensory pleasantness metric and an unbiased annoyance
metric <span class="citation" data-cites="boomy-sharpy">(Zwicker &amp;
Fastl, 2013)</span>. Using Zwicker and Fastl’s approach sharpness can be
calculated as:</p>
<p>where <em><span class="math inline"><em>N</em></span></em> is the
total spectral loudness, <em><span
class="math inline"><em>g</em>(<em>z</em>)</span></em> is the weighting
factor for sharpness as a function of the critical-band rate, and is a
scaling factor. Only for critical-band rates greater than 16 bark does
the weighting factor increase from unity to a value of 4 at the end of
the critical-band rate near 24 bark (Figure 7).</p>
<figure>
<img src="media/sharp.png"
alt="Weighting, g(z), as a function of critical band rate for sharpness" />
<figcaption aria-hidden="true">Weighting, g(z), as a function of
critical band rate for sharpness</figcaption>
</figure>
<p>The AudioCommons implementation differs slightly from the above
model. It windows the sound into frames of 4096 samples and then
calculates the loudness of all 1/3 octave bands within the window up to
the Nyquist frequency. The implementation in Max is similar, except that
it uses an FFT to calculate the sharpness of each frame up to
13,500Hz:</p>
<p>with:</p>
<p>where <em><span
class="math inline"><em>n</em>(<em>ω</em>)</span></em> is the bin number
relating to frequency <em><span
class="math inline"><em>ω</em></span></em>, <em><span
class="math inline"><em>x</em>(<em>n</em>)</span></em> is the magnitude
of the <em><span class="math inline"><em>n</em></span>th</em> bin,
<em><span
class="math inline"><em>v</em><sub><em>n</em></sub></span></em> is the
size of the FFT frame divided by 10, and <em><span
class="math inline"><em>g</em><em>z</em>(<em>n</em>)</span></em> is the
weighting factor for sharpness as a function of the critical-band rate.
Only for critical-band rates greater than 2899Hz does the weighting
factor increase from unity to a value of 4 at the end of the
critical-band rate near 13,500Hz.</p>
<h3 data-number="4.2.8" id="pnp.warmth"><span
class="header-section-number">4.2.8</span> pnp.warmth~</h3>
<p>A warm sound is one that promotes a sensation analogous to that
caused by a physical increase in temperature. Several methods for
calculating warmth have been proposed, all of which indicate that
concentrated low spectral energy correlates with the perception of
warmth. Pratt et al. proposes that a low spectral centroid and high
energy in the first three harmonics above the fundamental frequency
indicates that a sound is warm <span class="citation"
data-cites="warmth">(Pratt &amp; Doak, 1976)</span>. The pnp.warmth~
object calculates the apparent warmth of an incoming audio signal using
a direct implementation of the the model described by Pearce et al <span
class="citation" data-cites="acrelease">(Pearce &amp; Brookes,
2019)</span>. This model calculates the spectral centroid of the mean
warmth region:</p>
<p>where <em><span
class="math inline"><em>f</em><em>u</em><em>n</em><em>d</em></span></em>
is the fundamental frequency relating to the signal, <em><span
class="math inline"><em>n</em>(<em>ω</em>)</span></em> is the bin number
relating to frequency <em><span
class="math inline"><em>ω</em></span></em>, <em><span
class="math inline"><em>f</em>(<em>n</em>)</span></em> is the frequency
of the <em><span class="math inline"><em>n</em></span>th</em> bin, and
<em><span class="math inline"><em>x</em>(<em>n</em>)</span></em> is the
magnitude of the <em><span class="math inline"><em>n</em></span>th</em>
bin. The model also calculates the ratio of energy between the mean
warmth region compared to all energy up to the Nyquist frequency:</p>
<p>where <em><span
class="math inline"><em>n</em>(<em>N</em><em>y</em><em>q</em><em>u</em><em>i</em><em>s</em><em>t</em>)</span></em>
is the frequency relating to the Nyquist frequency.</p>
<h2 data-number="4.3" id="controls"><span
class="header-section-number">4.3</span> Controls</h2>
<p>The controls provide a wide range of functionality. Most objects in
this category modify output values from descriptors to provide users
with more control in terms of parameter automation or mapping. Several
of these objects remove either low or high values to prevent unwanted
noise, such as pnp.nozero and pnp.noone, while others smooth output
values to prevent rapid value changes. The controls also provide easing
functions to create non-linear mapping in reflexive systems.</p>
<figure>
<img src="media/autoscale.png"
alt="The pnp.autoscale~ implementation in Max" />
<figcaption aria-hidden="true">The pnp.autoscale~ implementation in
Max</figcaption>
</figure>
<p>Many of the sound descriptors are amplitude dependent, so they are
more likely to output higher values if the amplitude of the incoming
audio signal is higher. To prevent this, pnp.autoscale~. (Figure 8) is
designed to track the amplitude of the incoming signal and scale it
towards a target amplitude value. The object is a modified version of
the Adaptive Signal Level Scaling object proposed by Mikahil Malt and
Emmanuel Jordan <span class="citation" data-cites="zsaevents">(Malt
&amp; Jourdan, 2009)</span>. The main difference is that pnp.autoscale~
uses amplitude values from 0-1 as inputs, which allows it to be easily
controlled by other objects from the package. The second inlet (the
patch cable connected to the box labelled 2) sets the amplitude level to
maintain, while the third inlet specifies the trigger threshold. When
the amplitude value threshold from the third inlet is met, the object
will scale the signal to the amplitude level specified by the second
inlet.</p>
<h1 data-number="5" id="conclusion-and-future-work"><span
class="header-section-number">5</span> Conclusion and Future Work</h1>
<p>Reflexive Automation is an interdisciplinary method for computer
music automation that is defined as any system that moves or acts of
itself in relation to an external force. It involve the simultaneous use
of automation, mapping, and music information retrieval in a system and
describes automation as part of a cybernetic system where real-time
audio analysis tools implemented by the composer along with mapping
strategies dictate inputs. The PnP.Maxtools library is a library of
real-time objects for composers to build musical systems for music using
live electronics as well as a design pattern used as a blueprint for
using the library objects. The intimate relationship between the source
of the sound and the proceeding effect on that sound is its primary
point of concern, while reusability, customization, and abstraction is
its secondary point of concern. The PnP.Maxtools package in Max is
equipped with several categories: filters, descriptors, controls, and
effects. Objects within these categories can be used interchangeably
with others from the same category to build reflexive systems. The
newest additions to the Max programming environment have been detailed
along with research used when developing these models.</p>
<p>Future work will include creating a taxonomy of real-time MIR
applications for music composition utilizing the PnP.Maxtools package
and framework included in the PnP.Maxtools package launcher. This will
provide a base upon which members of the Max community can build. New
subjective descriptors, such as glass and wooden timbral estimators
alongside more common tools like chord, key, and chroma analysis, are in
development. By broadening the scope and applications of this software
through new tools and designs, the PnP.Maxtools package will become more
robust and better able to serve the needs of artists.</p>
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
data-line-spacing="2" role="list">
<div id="ref-alpern" class="csl-entry" role="listitem">
Alpern, A. (1995). Techniques for algorithmic composition of music.
<em>On the Web: Http://Hamp. Hampshire.
Edu/adaF92/Algocomp/Algocomp</em>, <em>95</em>(1995), 120.
</div>
<div id="ref-acfirst" class="csl-entry" role="listitem">
Andy Pearce, R. M., Tim Brookes. (2017). <em>First prototype of timbral
characterisation tools for semantically annotating non-musical
content</em>.
</div>
<div id="ref-controlling" class="csl-entry" role="listitem">
Aramaki, M., Besson, M., Kronland-Martinet, R., &amp; Ystad, S. (2010).
Controlling the perceived material in an impact sound synthesizer.
<em>IEEE Transactions on Audio, Speech, and Language Processing</em>,
<em>19</em>(2), 301–314.
</div>
<div id="ref-aures" class="csl-entry" role="listitem">
Aures, W. (1985). A model for calculating the sensory euphony of various
sounds. <em>Acustica</em>, <em>59</em>, 130–141.
</div>
<div id="ref-von" class="csl-entry" role="listitem">
Bismarck, G. von. (1974). Sharpness as an attribute of the timbre of
steady sounds. <em>Acta Acustica United with Acustica</em>,
<em>30</em>(3), 159–172.
</div>
<div id="ref-mvc" class="csl-entry" role="listitem">
Bucanek, J. (2009). Model-view-controller pattern. <em>Learn Objective-C
for Java Developers</em>, 353–402.
</div>
<div id="ref-cartwright" class="csl-entry" role="listitem">
Cartwright, M. B., &amp; Pardo, B. (2013). Social-EQ: Crowdsourcing an
equalization descriptor map. <em>ISMIR</em>, 395–400.
</div>
<div id="ref-cope" class="csl-entry" role="listitem">
Cope, D. H. (1976). <em>New directions in music. Dubuque, iowa: Wm.
c</em>. Brown Company Publishers.
</div>
<div id="ref-austin" class="csl-entry" role="listitem">
Franklin, A. A. (2022). PnP maxtools: Autonomous parameter control in
MaxMSP utilizing MIR algorithms. <em>LSU Doctoral Dissertations</em>,
81–111.
</div>
<div id="ref-freed" class="csl-entry" role="listitem">
Freed, D. J. (1990). Auditory correlates of perceived mallet hardness
for a set of recorded percussive sound events. <em>The Journal of the
Acoustical Society of America</em>, <em>87</em>(1), 311–322.
</div>
<div id="ref-booming-index" class="csl-entry" role="listitem">
Hatano, S., &amp; Hashimoto, T. (2000). Booming index as a measure for
evaluating booming sensation. <em>Proc. Inter-Noise</em>, 1–6.
</div>
<div id="ref-sdp" class="csl-entry" role="listitem">
Heer, J., &amp; Agrawala, M. (2006). Software design patterns for
information visualization. <em>IEEE Transactions on Visualization and
Computer Graphics</em>, <em>12</em>(5), 853–860.
</div>
<div id="ref-helmholtz" class="csl-entry" role="listitem">
Helmholtz, H. L. (2009). <em>On the sensations of tone as a
physiological basis for the theory of music</em>. Cambridge University
Press.
</div>
<div id="ref-hunt" class="csl-entry" role="listitem">
Hunt, A., &amp; Wanderley, M. M. (2002). “Mapping performer parameters
to synthesis engines." organised sound 7, no. 2 (2002): 97-108.
<em>Organised Sound</em>, <em>7</em>(2), 97–108.
</div>
<div id="ref-consonance" class="csl-entry" role="listitem">
Hutchinson, W., &amp; Knopoff, L. (1978). The acoustic component of
western consonance. <em>Journal of New Music Research</em>,
<em>7</em>(1), 1–29.
</div>
<div id="ref-flatness" class="csl-entry" role="listitem">
Izmirli, O. (2000). <em>Using a spectral flatness based feature for
audio segmentation and retrieval</em>.
</div>
<div id="ref-hilbert" class="csl-entry" role="listitem">
Johansson, M. (1999). The hilbert transform. <em>Mathematics Master’s
Thesis. V<span>ä</span>xj<span>ö</span> University, Suecia. Disponible
En Internet: Http://W3. Msi. Vxu. Se/Exarb/Mj_ex. Pdf, Consultado
El</em>, <em>19</em>.
</div>
<div id="ref-pluck" class="csl-entry" role="listitem">
Karjalainen, M., Välimäki, V., &amp; Tolonen, T. (1998). Plucked-string
models: From the karplus-strong algorithm to digital waveguides and
beyond. <em>Computer Music Journal</em>, <em>22</em>(3), 17–32.
</div>
<div id="ref-voyager" class="csl-entry" role="listitem">
Lewis, G. E. (2000). Too many notes: Computers, complexity and culture
in voyager. <em>Leonardo Music Journal</em>, <em>10</em>, 33–39.
</div>
<div id="ref-zsa" class="csl-entry" role="listitem">
Malt, M., &amp; Jourdan, E. (2008). Zsa. Descriptors: A library for
real-time descriptors analysis. <em>5th Sound and Music Computing
Conference, Berlin, Germany</em>, 134–137.
</div>
<div id="ref-zsaevents" class="csl-entry" role="listitem">
Malt, M., &amp; Jourdan, E. (2009). Real-time uses of low level sound
descriptors as event detection functions using the max/msp zsa.
Descriptors library. <em>Proceedings of the 12th Brazilian Smposium on
Computer Music</em>.
</div>
<div id="ref-cranfield" class="csl-entry" role="listitem">
Moffat, D., Ronan, D., &amp; Reiss, J. D. (2015). <em>An evaluation of
audio feature extraction toolboxes</em>.
</div>
<div id="ref-acrelease" class="csl-entry" role="listitem">
Pearce, &amp; Brookes. (2019). <em>Release of timbral characterisation
tools for semantically annotating non-musical content</em>.
</div>
<div id="ref-tonal" class="csl-entry" role="listitem">
Plomp, R., &amp; Levelt, W. J. M. (1965). Tonal consonance and critical
bandwidth. <em>The Journal of the Acoustical Society of America</em>,
<em>38</em>(4), 548–560.
</div>
<div id="ref-warmth" class="csl-entry" role="listitem">
Pratt, R., &amp; Doak, P. E. (1976). A subjective rating scale for
timbre. <em>Journal of Sound and Vibration</em>, <em>45</em>(3),
317–328.
</div>
<div id="ref-fiddle" class="csl-entry" role="listitem">
Puckette, M. S., Ucsd, M. S. P., Apel, T., et al. (1998). <em>Real-time
audio analysis tools for pd and MSP</em>.
</div>
<div id="ref-mubu" class="csl-entry" role="listitem">
Schnell, N., Röbel, A., Schwarz, D., Peeters, G., Borghesi, R., et al.
(2009). MuBu and friends–assembling tools for content based real-time
interactive audio processing in max/MSP. <em>ICMC</em>.
</div>
<div id="ref-gabor" class="csl-entry" role="listitem">
Schnell, N., &amp; Schwarz, D. (2005). Gabor, multi-representation
real-time analysis/synthesis. <em>COST-G6 Conference on Digital Audio
Effects (DAFx)</em>, 122–126.
</div>
<div id="ref-cybernetics" class="csl-entry" role="listitem">
Steer, M. (1952). <em>Science</em>, <em>115</em>(2978), 100–100.
</div>
<div id="ref-indirect" class="csl-entry" role="listitem">
Traube, C., Depalle, P., &amp; Wanderley, M. (2003). Indirect
acquisition of instrumental gesture based on signal, physical and
perceptual information. <em>Proceedings of the 2003 Conference on New
Interfaces for Musical Expression</em>, 42–47.
</div>
<div id="ref-roughness" class="csl-entry" role="listitem">
Vassilakis, P. N., &amp; Fitz, K. (2007). SRA: A web-based research tool
for spectral and roughness analysis of sound signals. <em>Proceedings of
the 4th Sound and Music Computing (SMC) Conference</em>, 319–325.
</div>
<div id="ref-wiener" class="csl-entry" role="listitem">
Wiener, N. (2019). <em>Cybernetics or control and communication in the
animal and the machine</em>. MIT press.
</div>
<div id="ref-williams" class="csl-entry" role="listitem">
Williams, D. (2010). <em>Towards a timbre morpher</em>. University of
Surrey (United Kingdom).
</div>
<div id="ref-flucoma" class="csl-entry" role="listitem">
Zbyszynski, M. (2019). <em>Instruments &amp; situations: Machine
learning, synthesis &amp; creative coding</em>.
</div>
<div id="ref-boomy-sharpy" class="csl-entry" role="listitem">
Zwicker, E., &amp; Fastl, H. (2013). <em>Psychoacoustics: Facts and
models</em> (Vol. 22). Springer Science &amp; Business Media.
</div>
</div>
</article>
</body>
</html>
